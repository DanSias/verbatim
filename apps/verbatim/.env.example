# Database (Postgres with pgvector)
DATABASE_URL="postgresql://user:password@localhost:5432/verbatim?schema=public"

# LLM Provider Configuration
# Default provider: gemini | openai | anthropic
LLM_PROVIDER=gemini

# Provider API Keys (set the one for your chosen provider)
GOOGLE_API_KEY=""
OPENAI_API_KEY=""
ANTHROPIC_API_KEY=""

# Optional: Override default models per provider
# LLM_MODEL_GEMINI=gemini-2.0-flash
# LLM_MODEL_OPENAI=gpt-4o-mini
# LLM_MODEL_ANTHROPIC=claude-sonnet-4-20250514

# LLM Defensive Caps (Phase 7.1)
# Timeout for LLM API calls in milliseconds
LLM_TIMEOUT_MS=20000
# Maximum output tokens
LLM_MAX_TOKENS=700
# Default temperature (0-1)
LLM_TEMPERATURE=0.2
# Maximum chunks to include in LLM prompt
LLM_MAX_CHUNKS=6
# Maximum characters per chunk excerpt
LLM_MAX_EXCERPT_CHARS=1200

# Rate Limiting (Phase 7.1 - pilot-safe in-memory)
# Window duration in milliseconds
RATE_LIMIT_WINDOW_MS=60000
# Maximum requests per window per IP
RATE_LIMIT_MAX_REQUESTS=60

# Feature flags
ENABLE_FRESHDESK_TICKETS=false

# Freshdesk (for future use)
# FRESHDESK_DOMAIN=""
# FRESHDESK_API_KEY=""

# Debug mode
DEBUG=false
