# Database (Postgres with pgvector)
DATABASE_URL="postgresql://user:password@localhost:5432/verbatim?schema=public"

# LLM Provider Configuration
# Default provider: gemini | openai | anthropic
LLM_PROVIDER=gemini

# Provider API Keys (set the one for your chosen provider)
GOOGLE_API_KEY=""
OPENAI_API_KEY=""
ANTHROPIC_API_KEY=""

# Optional: Override default models per provider
# LLM_MODEL_GEMINI=gemini-2.0-flash
# LLM_MODEL_OPENAI=gpt-4o-mini
# LLM_MODEL_ANTHROPIC=claude-sonnet-4-20250514

# LLM Defensive Caps (Phase 7.1)
# Timeout for LLM API calls in milliseconds
LLM_TIMEOUT_MS=20000
# Maximum output tokens
LLM_MAX_TOKENS=700
# Default temperature (0-1)
LLM_TEMPERATURE=0.2
# Maximum chunks to include in LLM prompt
LLM_MAX_CHUNKS=6
# Maximum characters per chunk excerpt
LLM_MAX_EXCERPT_CHARS=1200

# Rate Limiting (Phase 7.1 - pilot-safe in-memory)
# Window duration in milliseconds
RATE_LIMIT_WINDOW_MS=60000
# Maximum requests per window per IP
RATE_LIMIT_MAX_REQUESTS=60

# Widget Configuration (Phase 8.1, 8.2)
# Required: workspace ID for widget requests
WIDGET_DEFAULT_WORKSPACE_ID=
# Optional: default corpus scope (comma-separated: docs,kb)
WIDGET_DEFAULT_CORPUS_SCOPE=docs,kb
# Optional: default minimum confidence (low|medium|high)
WIDGET_DEFAULT_MIN_CONFIDENCE=medium
# Optional: default LLM provider (gemini|openai|anthropic)
WIDGET_DEFAULT_PROVIDER=
# Enable widget UI (set to 1 to show, 0 to hide)
NEXT_PUBLIC_WIDGET_ENABLED=0

# Widget Upstream Mode (Phase 8.2)
# local: proxy to local /api/answer (default, used in Verbatim)
# remote: proxy to external VERBATIM_BASE_URL/api/answer (used in docs repo)
WIDGET_UPSTREAM_MODE=local
# Required when WIDGET_UPSTREAM_MODE=remote
VERBATIM_BASE_URL=
# Optional: API key for authenticated requests to remote Verbatim instance
VERBATIM_API_KEY=

# Query Event Logging (Phase 8.3)
# Log full question text (PII risk - default off)
LOG_QUESTION_TEXT=0
# Number of characters for question preview (default 120)
LOG_QUESTION_PREVIEW_CHARS=120
# Enable cost tracking (default on)
COST_TRACKING_ENABLED=1
# Optional: custom model pricing JSON (overrides defaults)
# Format: { "model-name": { "inputPer1M": 1.0, "outputPer1M": 2.0 } }
# COST_MODEL_PRICING_JSON=

# Feature flags
ENABLE_FRESHDESK_TICKETS=false

# Freshdesk (for future use)
# FRESHDESK_DOMAIN=""
# FRESHDESK_API_KEY=""

# Debug mode
DEBUG=false
